# -*- coding: utf-8 -*-
"""s_d_ecommerce_furniture_analysis_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12MRW7rfEozKG6QpMDXBa-5Rv8ZauwVom

# E-Commerce Furniture Sales Analysis

**Dataset:** 2,000 furniture products scraped from AliExpress  
**Goal:** Figure out what drives sales and build a model to predict them

---

## 1. Setup & Data Loading
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# plotting style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 11

print('Libraries loaded')

# data loading
df = pd.read_csv('/content/ecommerce_furniture_dataset_2024.csv')
print(f'Loaded {len(df):,} products')
df.head()

"""## 2. Data Cleaning

The data is messy - prices have dollar signs, lots of missing values in originalPrice
"""

# data types and missing values
print('Data types:')
print(df.dtypes)
print('\nMissing values:')
print(df.isnull().sum())
print(f'\nMissing originalPrice: {df["originalPrice"].isna().sum()} ({df["originalPrice"].isna().mean()*100:.1f}%)')

# clean up the price columns
def clean_price(x):
    if pd.isna(x) or x == '':
        return np.nan
    return float(str(x).replace('$', '').replace(',', ''))

df['price_clean'] = df['price'].apply(clean_price)
df['original_price_clean'] = df['originalPrice'].apply(clean_price)

# check the results
print(f'Price range: ${df["price_clean"].min():.2f} - ${df["price_clean"].max():.2f}')
print(f'Average price: ${df["price_clean"].mean():.2f}')
print(f'Median price: ${df["price_clean"].median():.2f}')

# calculate discount info
df['has_discount'] = df['original_price_clean'].notna()
df['discount_pct'] = np.where(
    df['has_discount'],
    (df['original_price_clean'] - df['price_clean']) / df['original_price_clean'] * 100,
    0
)

print(f'Products with discount: {df["has_discount"].sum()} ({df["has_discount"].mean()*100:.1f}%)')
print(f'Average discount (when present): {df.loc[df["has_discount"], "discount_pct"].mean():.1f}%')

# shipping info
df['free_shipping'] = df['tagText'].str.contains('Free shipping', na=False).astype(int)
print(f'Free shipping: {df["free_shipping"].sum()} products ({df["free_shipping"].mean()*100:.1f}%)')

# extract product categories from titles
categories = ['chair', 'table', 'sofa', 'bed', 'dresser', 'desk', 'cabinet', 'wardrobe', 'shelf', 'stool']

def get_category(title):
    title_lower = str(title).lower()
    for cat in categories:
        if cat in title_lower:
            return cat.title()
    return 'Other'

df['category'] = df['productTitle'].apply(get_category)
print('Category distribution:')
print(df['category'].value_counts())

# title features - maybe longer titles perform better?
df['title_length'] = df['productTitle'].str.len()
df['title_word_count'] = df['productTitle'].str.split().str.len()

print(f'Average title length: {df["title_length"].mean():.0f} characters')
print(f'Average word count: {df["title_word_count"].mean():.0f} words')

# price tiers
def price_tier(price):
    if price < 50:
        return 'Budget (<$50)'
    elif price < 150:
        return 'Mid-Range ($50-150)'
    elif price < 300:
        return 'Premium ($150-300)'
    else:
        return 'Luxury ($300+)'

df['price_tier'] = df['price_clean'].apply(price_tier)
print('Price tier distribution:')
print(df['price_tier'].value_counts())

"""## 3. Exploratory Data Analysis

Let's see what patterns emerge from the data

### 3.1 Sales Distribution
"""

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# raw distribution
axes[0].hist(df['sold'], bins=50, color='steelblue', edgecolor='white')
axes[0].set_xlabel('Units Sold')
axes[0].set_ylabel('Number of Products')
axes[0].set_title('Sales Distribution (Raw)')
axes[0].axvline(df['sold'].mean(), color='red', linestyle='--', label=f'Mean: {df["sold"].mean():.1f}')
axes[0].axvline(df['sold'].median(), color='orange', linestyle='--', label=f'Median: {df["sold"].median():.0f}')
axes[0].legend()

# log scale to see the tail
df_nonzero = df[df['sold'] > 0]
axes[1].hist(np.log1p(df_nonzero['sold']), bins=30, color='steelblue', edgecolor='white')
axes[1].set_xlabel('Log(Units Sold + 1)')
axes[1].set_ylabel('Number of Products')
axes[1].set_title('Sales Distribution (Log Scale, excluding zeros)')

plt.tight_layout()
plt.savefig('01_sales_distribution.png', dpi=150, bbox_inches='tight')
plt.show()

print(f'\nSales stats:')
print(f'  Total units sold: {df["sold"].sum():,}')
print(f'  Mean: {df["sold"].mean():.1f}')
print(f'  Median: {df["sold"].median():.0f}')
print(f'  Max: {df["sold"].max():,}')
print(f'  Products with 0 sales: {(df["sold"]==0).sum()} ({(df["sold"]==0).mean()*100:.1f}%)')

"""### 3.2 Price Analysis"""

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# price distribution
axes[0].hist(df['price_clean'], bins=50, color='teal', edgecolor='white')
axes[0].set_xlabel('Price ($)')
axes[0].set_ylabel('Number of Products')
axes[0].set_title('Price Distribution')
axes[0].axvline(df['price_clean'].median(), color='red', linestyle='--', label=f'Median: ${df["price_clean"].median():.0f}')
axes[0].legend()

# price vs sales
axes[1].scatter(df['price_clean'], df['sold'], alpha=0.3, color='teal')
axes[1].set_xlabel('Price ($)')
axes[1].set_ylabel('Units Sold')
axes[1].set_title('Price vs Sales')

plt.tight_layout()
plt.savefig('02_price_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

# sales by price tier
tier_order = ['Budget (<$50)', 'Mid-Range ($50-150)', 'Premium ($150-300)', 'Luxury ($300+)']
tier_stats = df.groupby('price_tier')['sold'].agg(['mean', 'median', 'sum', 'count']).reindex(tier_order)

fig, ax = plt.subplots(figsize=(10, 5))
bars = ax.bar(tier_stats.index, tier_stats['mean'], color=['#2ecc71', '#3498db', '#9b59b6', '#e74c3c'])
ax.set_xlabel('Price Tier')
ax.set_ylabel('Average Units Sold')
ax.set_title('Average Sales by Price Tier')

# add value labels
for bar, val in zip(bars, tier_stats['mean']):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}',
            ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig('03_sales_by_price_tier.png', dpi=150, bbox_inches='tight')
plt.show()

print('Sales by Price Tier:')
print(tier_stats.round(1))

"""### 3.3 Category Performance"""

cat_stats = df.groupby('category')['sold'].agg(['mean', 'sum', 'count']).sort_values('sum', ascending=False)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# total sales by category
colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(cat_stats)))
axes[0].barh(cat_stats.index[::-1], cat_stats['sum'][::-1], color=colors[::-1])
axes[0].set_xlabel('Total Units Sold')
axes[0].set_title('Total Sales by Category')

# average sales by category
cat_avg = cat_stats.sort_values('mean', ascending=False)
axes[1].barh(cat_avg.index[::-1], cat_avg['mean'][::-1], color=colors[::-1])
axes[1].set_xlabel('Average Units Sold per Product')
axes[1].set_title('Average Sales by Category')

plt.tight_layout()
plt.savefig('04_category_performance.png', dpi=150, bbox_inches='tight')
plt.show()

print('Category Performance:')
print(cat_stats.round(1))

"""### 3.4 The Discount Effect

This is the big one - does discounting actually work?
"""

discount_comparison = df.groupby('has_discount')['sold'].agg(['mean', 'median', 'sum', 'count'])
discount_comparison.index = ['No Discount', 'Has Discount']

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# bar chart
bars = axes[0].bar(['No Discount', 'Has Discount'], discount_comparison['mean'],
                   color=['#e74c3c', '#2ecc71'], edgecolor='white', linewidth=2)
axes[0].set_ylabel('Average Units Sold')
axes[0].set_title('Average Sales: Discount vs No Discount')
for bar, val in zip(bars, discount_comparison['mean']):
    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}',
                ha='center', va='bottom', fontweight='bold', fontsize=14)

# discount percentage vs sales (for discounted items only)
discounted = df[df['has_discount']]
axes[1].scatter(discounted['discount_pct'], discounted['sold'], alpha=0.5, color='teal')
axes[1].set_xlabel('Discount Percentage (%)')
axes[1].set_ylabel('Units Sold')
axes[1].set_title('Discount % vs Sales (Discounted Products Only)')

plt.tight_layout()
plt.savefig('05_discount_effect.png', dpi=150, bbox_inches='tight')
plt.show()

# the key finding
lift = discount_comparison.loc['Has Discount', 'mean'] / discount_comparison.loc['No Discount', 'mean']
print(f'\n*** KEY FINDING ***')
print(f'Discounted products sell {lift:.1f}x more on average!')
print(f'  - No discount: {discount_comparison.loc["No Discount", "mean"]:.1f} avg sales')
print(f'  - With discount: {discount_comparison.loc["Has Discount", "mean"]:.1f} avg sales')
print(f'\nBut only {df["has_discount"].mean()*100:.1f}% of products have discounts!')

"""### 3.5 Free Shipping Impact"""

shipping_stats = df.groupby('free_shipping')['sold'].agg(['mean', 'median', 'count'])
shipping_stats.index = ['Paid Shipping', 'Free Shipping']

fig, ax = plt.subplots(figsize=(8, 5))
bars = ax.bar(shipping_stats.index, shipping_stats['mean'], color=['#e67e22', '#27ae60'])
ax.set_ylabel('Average Units Sold')
ax.set_title('Free Shipping Impact on Sales')

for bar, val in zip(bars, shipping_stats['mean']):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.1f}',
            ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig('06_shipping_impact.png', dpi=150, bbox_inches='tight')
plt.show()

print('Shipping Stats:')
print(shipping_stats)
print(f'\nFree shipping is offered on {df["free_shipping"].mean()*100:.0f}% of products')
print('So it\'s basically table stakes at this point - not a differentiator')

"""### 3.6 Correlation Analysis"""

# numeric features for correlation
numeric_cols = ['price_clean', 'sold', 'has_discount', 'discount_pct', 'free_shipping',
                'title_length', 'title_word_count']

corr_matrix = df[numeric_cols].corr()

fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, fmt='.2f',
            square=True, linewidths=0.5, ax=ax)
ax.set_title('Feature Correlation Matrix')
plt.tight_layout()
plt.savefig('07_correlation_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

print('Correlations with Sales:')
print(corr_matrix['sold'].sort_values(ascending=False).round(3))

"""### 3.7 Top Sellers"""

top_10 = df.nlargest(10, 'sold')[['productTitle', 'price_clean', 'sold', 'has_discount', 'category']]
top_10['productTitle'] = top_10['productTitle'].str[:60] + '...'

print('Top 10 Best Sellers:')
print('='*100)
for i, row in top_10.iterrows():
    discount_flag = 'ðŸ·ï¸' if row['has_discount'] else ''
    print(f"{row['sold']:,} sold | ${row['price_clean']:.2f} | {row['category']} {discount_flag}")
    print(f"  {row['productTitle']}")
    print()

"""## 4. Feature Engineering for ML

Now let's prepare features for the prediction model
"""

# create the feature set
df_ml = df.copy()

# encode category
le = LabelEncoder()
df_ml['category_encoded'] = le.fit_transform(df_ml['category'])

# price tier encoding
tier_map = {'Budget (<$50)': 0, 'Mid-Range ($50-150)': 1, 'Premium ($150-300)': 2, 'Luxury ($300+)': 3}
df_ml['price_tier_encoded'] = df_ml['price_tier'].map(tier_map)

# log transform price (helps with skewed distribution)
df_ml['log_price'] = np.log1p(df_ml['price_clean'])

# features list
feature_cols = [
    'price_clean', 'log_price', 'has_discount', 'discount_pct', 'free_shipping',
    'title_length', 'title_word_count', 'category_encoded', 'price_tier_encoded'
]

print(f'Features: {len(feature_cols)}')
for col in feature_cols:
    print(f'  - {col}')

# prepare X and y
X = df_ml[feature_cols].copy()
y = df_ml['sold'].copy()

# handle any missing values
X = X.fillna(0)

# log transform target (sales are heavily right-skewed)
y_log = np.log1p(y)

print(f'X shape: {X.shape}')
print(f'y shape: {y.shape}')
print(f'\nTarget stats (original):')
print(f'  Mean: {y.mean():.1f}, Std: {y.std():.1f}')
print(f'\nTarget stats (log transformed):')
print(f'  Mean: {y_log.mean():.2f}, Std: {y_log.std():.2f}')

# train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.2, random_state=42)

# scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f'Training set: {len(X_train)} samples')
print(f'Test set: {len(X_test)} samples')

"""## 5. Model Training & Evaluation"""

# define models to try
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
}

results = []

for name, model in models.items():
    print(f'Training {name}...')

    # use scaled data for linear models
    if 'Linear' in name or 'Ridge' in name or 'Lasso' in name:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    # metrics (on log scale)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    # convert back to original scale for interpretable MAE
    y_test_orig = np.expm1(y_test)
    y_pred_orig = np.expm1(y_pred)
    mae_orig = mean_absolute_error(y_test_orig, y_pred_orig)

    results.append({
        'Model': name,
        'R2': r2,
        'RMSE (log)': rmse,
        'MAE (units)': mae_orig
    })

results_df = pd.DataFrame(results).sort_values('R2', ascending=False)
print('\n' + '='*60)
print('MODEL COMPARISON')
print('='*60)
print(results_df.to_string(index=False))

# visualize model comparison
fig, ax = plt.subplots(figsize=(10, 5))
colors = ['#2ecc71' if x == results_df['R2'].max() else '#3498db' for x in results_df['R2']]
bars = ax.barh(results_df['Model'], results_df['R2'], color=colors)
ax.set_xlabel('RÂ² Score')
ax.set_title('Model Comparison (Higher is Better)')
ax.set_xlim(0, max(results_df['R2']) * 1.15)

for bar, val in zip(bars, results_df['R2']):
    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.3f}',
            va='center', fontweight='bold')

plt.tight_layout()
plt.savefig('08_model_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

"""### 5.1 Feature Importance (Random Forest)"""

# get feature importance from the best model
rf_model = models['Random Forest']
importance = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

fig, ax = plt.subplots(figsize=(10, 6))
colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance)))
bars = ax.barh(importance['Feature'][::-1], importance['Importance'][::-1], color=colors[::-1])
ax.set_xlabel('Importance')
ax.set_title('Feature Importance (Random Forest)')

for bar, val in zip(bars, importance['Importance'][::-1]):
    ax.text(val + 0.005, bar.get_y() + bar.get_height()/2, f'{val:.1%}',
            va='center', fontsize=10)

plt.tight_layout()
plt.savefig('09_feature_importance.png', dpi=150, bbox_inches='tight')
plt.show()

print('Feature Importance:')
for _, row in importance.iterrows():
    print(f"  {row['Feature']:25s} {row['Importance']:6.1%}")

"""### 5.2 Prediction Analysis"""

# actual vs predicted
y_pred_rf = rf_model.predict(X_test)
y_test_orig = np.expm1(y_test)
y_pred_orig = np.expm1(y_pred_rf)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# scatter plot
axes[0].scatter(y_test_orig, y_pred_orig, alpha=0.4, color='teal')
max_val = max(y_test_orig.max(), y_pred_orig.max())
axes[0].plot([0, max_val], [0, max_val], 'r--', label='Perfect prediction')
axes[0].set_xlabel('Actual Sales')
axes[0].set_ylabel('Predicted Sales')
axes[0].set_title('Actual vs Predicted Sales')
axes[0].legend()

# residuals
residuals = y_test_orig - y_pred_orig
axes[1].hist(residuals, bins=50, color='steelblue', edgecolor='white')
axes[1].axvline(0, color='red', linestyle='--')
axes[1].set_xlabel('Residual (Actual - Predicted)')
axes[1].set_ylabel('Frequency')
axes[1].set_title('Residual Distribution')

plt.tight_layout()
plt.savefig('10_prediction_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

print(f'Mean Absolute Error: {mean_absolute_error(y_test_orig, y_pred_orig):.1f} units')
print(f'Median Absolute Error: {np.median(np.abs(residuals)):.1f} units')

"""### 5.3 Cross-Validation"""

# cross-validation for more robust estimate
cv_scores = cross_val_score(rf_model, X, y_log, cv=5, scoring='r2')

print('5-Fold Cross-Validation Results (Random Forest):')
print(f'  RÂ² scores: {cv_scores.round(3)}')
print(f'  Mean RÂ²: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})')

"""## 6. Key Findings Summary"""

print('='*70)
print('KEY FINDINGS')
print('='*70)

print('\nðŸ“Š DATASET OVERVIEW')
print(f'   â€¢ {len(df):,} products analyzed')
print(f'   â€¢ {df["sold"].sum():,} total units sold')
print(f'   â€¢ Average price: ${df["price_clean"].mean():.2f}')
print(f'   â€¢ Median sales: {df["sold"].median():.0f} units')

print('\nðŸ’° DISCOUNT IMPACT (Most Important Finding!)')
no_disc = df[~df['has_discount']]['sold'].mean()
with_disc = df[df['has_discount']]['sold'].mean()
print(f'   â€¢ Products WITH discount: {with_disc:.1f} avg sales')
print(f'   â€¢ Products WITHOUT discount: {no_disc:.1f} avg sales')
print(f'   â€¢ Lift: {with_disc/no_disc:.1f}x more sales with discounts!')
print(f'   â€¢ But only {df["has_discount"].mean()*100:.1f}% of products have discounts')

print('\nðŸ’µ PRICE SWEET SPOT')
for tier in tier_order:
    avg = df[df['price_tier']==tier]['sold'].mean()
    print(f'   â€¢ {tier}: {avg:.1f} avg sales')

print('\nðŸª‘ CATEGORY WINNERS')
top_cats = cat_stats.head(3)
for cat, row in top_cats.iterrows():
    print(f'   â€¢ {cat}: {row["sum"]:,.0f} total sales ({row["mean"]:.1f} avg)')

print('\nðŸ¤– MODEL PERFORMANCE')
best = results_df.iloc[0]
print(f'   â€¢ Best model: {best["Model"]}')
print(f'   â€¢ RÂ² Score: {best["R2"]:.3f} ({best["R2"]*100:.1f}% variance explained)')
print(f'   â€¢ Mean Absolute Error: {best["MAE (units)"]:.1f} units')

print('\nðŸŽ¯ TOP PREDICTIVE FEATURES')
for _, row in importance.head(4).iterrows():
    print(f'   â€¢ {row["Feature"]}: {row["Importance"]*100:.1f}%')

"""## 7. Business Recommendations"""

print('='*70)
print('BUSINESS RECOMMENDATIONS')
print('='*70)

print('''
1. IMPLEMENT STRATEGIC DISCOUNTING
   - Discounts drive 9.8x more sales, but only 23% of products use them
   - Recommendation: Test 40-50% discounts on slow-moving inventory
   - Expected impact: Significant sales lift with minimal margin impact

2. OPTIMIZE PRICE POSITIONING
   - Budget items (<$50) sell 20x more than luxury ($300+)
   - Sweet spot appears to be $30-$100
   - Recommendation: Shift inventory mix toward mid-range products

3. FOCUS ON WINNING CATEGORIES
   - Chairs and Tables account for 70% of total sales
   - Recommendation: Expand variety in these categories
   - Deprioritize underperforming categories

4. IMPROVE PRODUCT TITLES
   - Title length is a significant predictor (13% importance)
   - Recommendation: Write detailed titles (80-120 characters)
   - Include key features, materials, and use cases

5. MAINTAIN FREE SHIPPING
   - 94% of products offer it - it's table stakes now
   - No significant advantage, but removing it would hurt
''')

"""## 8. Save Outputs"""

# save cleaned data
df_ml.to_csv('data_cleaned.csv', index=False)
print('Saved: data_cleaned.csv')

# save model results
results_df.to_csv('model_results.csv', index=False)
print('Saved: model_results.csv')

# save feature importance
importance.to_csv('feature_importance.csv', index=False)
print('Saved: feature_importance.csv')

print('\nAll outputs saved!')

# save the best model
import pickle

with open('best_model_rf.pkl', 'wb') as f:
    pickle.dump(rf_model, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print('Saved: best_model_rf.pkl')
print('Saved: scaler.pkl')

"""---

## Conclusion

The analysis reveals that **discounting is by far the most powerful lever** for driving furniture sales on AliExpress. With only 23% of products currently discounted, there's a massive opportunity to boost sales through strategic discount programs.

Price positioning matters too - budget items dominate, and the $30-$100 range seems optimal for balancing volume and margin.

The ML model explains about 34% of sales variance, which is decent but not great. This suggests there are important factors we can't see in this data - likely product photos, seller reputation, reviews, and seasonal effects.

**Next steps would be:**
- Get more data (reviews, seller metrics, images)
- A/B test discount strategies
- Build a recommendation system for optimal pricing
"""